{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOob+L5CbxB+qAbXOjynoV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizjwh/groupIAI_5take2/blob/main/5%20Completely%20New%20Model/CommpletelyNewModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cNDCl6vme0RF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b71e1993-1e55-45bf-f070-715d9792170a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "#install torchvision and kaggle\n",
        "!pip install torchvision\n",
        "!pip install kaggle\n",
        "!pip install tqdm\n",
        "!pip install colorama\n",
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torchvision import transforms, models ,datasets\n",
        "from torch.utils.data import ConcatDataset\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "import os\n",
        "import splitfolders"
      ],
      "metadata": {
        "id": "UxU7_dxbgvN8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! export KAGGLE_USERNAME=\"emmet454\" && export KAGGLE_KEY=\"ee00fbc0728a71f5c5f712029e3ef004\" && kaggle datasets download --force --unzip emilyburt/intro-to-ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOAmwGpFgryu",
        "outputId": "c536ea98-938d-4cf3-b4dc-50a1ba56117e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading intro-to-ai.zip to /content\n",
            "100% 4.08G/4.08G [02:34<00:00, 30.9MB/s]\n",
            "100% 4.08G/4.08G [02:34<00:00, 28.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the folders into train and test"
      ],
      "metadata": {
        "id": "_dVREcJQkMWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = 'Dataset/Full Mobility/Left hand'\n",
        "\n",
        "#split with a ratio (train, val, test)\n",
        "splitfolders.ratio(input_folder, output='Full Mobility/Left hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Full Mobility/Right hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Full Mobility/Right hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Restricted mobility/Left hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Restricted Mobility/Left hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Restricted mobility/Right hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Restricted Mobility/Right hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4xqFUpkkOQE",
        "outputId": "72658573-9926-4a6b-c200-bb957b211525"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 492 files [00:11, 41.27 files/s]\n",
            "Copying files: 502 files [00:11, 45.41 files/s]\n",
            "Copying files: 484 files [00:10, 45.31 files/s]\n",
            "Copying files: 495 files [00:10, 47.75 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(root_dir):\n",
        "    \"\"\"\n",
        "    Creates an ImageFolder dataset from the specified root directory.\n",
        "    \"\"\"\n",
        "    data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.RandomRotation(30),\n",
        "                                          transforms.RandomHorizontalFlip(p=0.3),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "    dataset = datasets.ImageFolder(root_dir, transform=data_transforms)\n",
        "    return dataset\n",
        "\n",
        "def combine_datasets(root_folders):\n",
        "    \"\"\"\n",
        "    Combines datasets from multiple root folders into one dataset.\n",
        "    \"\"\"\n",
        "    datasets = [create_dataset(folder) for folder in root_folders]\n",
        "    combined_dataset = ConcatDataset(datasets)\n",
        "\n",
        "    # Extract class-to-index mappings from individual datasets and merge them\n",
        "    class_to_idx = {}\n",
        "    for dataset in datasets:\n",
        "        class_to_idx.update(dataset.class_to_idx)\n",
        "\n",
        "    return combined_dataset, class_to_idx\n",
        "\n",
        "\n",
        "def main():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/train',\n",
        "                    '/content/Restricted Mobility/Right hand/train',\n",
        "                    '/content/Full Mobility/Left hand/train',\n",
        "                    '/content/Full Mobility/Right hand/train']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "def main_val():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/val',\n",
        "                    '/content/Restricted Mobility/Right hand/val',\n",
        "                    '/content/Full Mobility/Left hand/val',\n",
        "                    '/content/Full Mobility/Right hand/val']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "def main_test():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/test',\n",
        "                    '/content/Restricted Mobility/Right hand/test',\n",
        "                    '/content/Full Mobility/Left hand/test',\n",
        "                    '/content/Full Mobility/Right hand/test']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "combined_train_dataset = main()\n",
        "combined_val_dataset = main_val()\n",
        "combined_test_dataset = main_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ4wK2JUolDm",
        "outputId": "afc0619c-f193-411f-a159-cb55ce658623"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class to index mapping: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'space': 26}\n",
            "Class to index mapping: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'space': 26}\n",
            "Class to index mapping: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'space': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create our completely new model with Squeezenet1_1"
      ],
      "metadata": {
        "id": "oC9N8KNDhxbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.squeezenet1_1(pretrained=True)\n",
        "\n",
        "# # Freeze parameters of the tarined network\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "#print the model to check the classifer and change it\n",
        "print (model.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNgzBZ41h4di",
        "outputId": "40a08fb9-d86a-4f2a-a6ab-3402ceca865f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define new classifier and append it to network but remember to have a 29-neuron output layer for our two classes.\n",
        "model.classifier= nn.Sequential(nn.Dropout(p=0.6, inplace=False),\n",
        "                                nn.Linear(in_features=1280, out_features=27, bias=True),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "#print the classifier now\n",
        "print(model.classifier)\n",
        "\n",
        "# choose your loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# define optimizer to train only the classifier and the previous three block.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# define Learning Rate scheduler to decrease the learning rate by multiplying it by 0.1 after each epoch on the data.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "#print the whole model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgLJ7dGCphxa",
        "outputId": "e353c07d-988b-40fe-dbc8-c156fdf2c54f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.6, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=27, bias=True)\n",
            "  (2): LogSoftmax(dim=1)\n",
            ")\n",
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (6): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.6, inplace=False)\n",
            "    (1): Linear(in_features=1280, out_features=27, bias=True)\n",
            "    (2): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configure training params and start training"
      ],
      "metadata": {
        "id": "x3Yw_MICrBIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define number of epochs through data and run the training loop\n",
        "import math\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(combined_train_dataset, batch_size=512, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(combined_test_dataset, batch_size=512)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "epochs = 5\n",
        "step = 0\n",
        "running_loss = 0\n",
        "print_every = 20\n",
        "trainlossarr=[]\n",
        "testlossarr=[]\n",
        "oldacc=0\n",
        "\n",
        "steps=math.ceil(len(combined_train_dataset)/(trainloader.batch_size))"
      ],
      "metadata": {
        "id": "ltGkQ5bDrEad"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start training loop"
      ],
      "metadata": {
        "id": "XbgGLZtnrIT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "from colorama import Fore,Style\n",
        "import sys\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(Style.RESET_ALL)\n",
        "    print(f\"--------------------------------- START OF EPOCH [ {epoch+1} ] >>> LR =  {optimizer.param_groups[-1]['lr']} ---------------------------------\\n\")\n",
        "    for inputs, labels in tqdm(trainloader,desc=Fore.GREEN +f\"* PROGRESS IN EPOCH {epoch+1} \",file=sys.stdout):\n",
        "        model.train()\n",
        "        step += 1\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        props = model.forward(inputs)\n",
        "        loss = criterion(props, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (step % print_every == 0) or (step==steps):\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in testloader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    props = model.forward(inputs)\n",
        "                    batch_loss = criterion(props, labels)\n",
        "\n",
        "                    test_loss += batch_loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    ps = torch.exp(props)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            tqdm.write(f\"Epoch ({epoch+1} of {epochs}) ... \"\n",
        "                  f\"Step  ({step:3d} of {steps}) ... \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f} ... \"\n",
        "                  f\"Test loss: {test_loss/len(testloader):.3f} ... \"\n",
        "                  f\"Test accuracy: {accuracy/len(testloader):.3f} \")\n",
        "            trainlossarr.append(running_loss/print_every)\n",
        "            testlossarr.append(test_loss/len(testloader))\n",
        "            running_loss = 0\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "    step=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow4g3EnVrJ1M",
        "outputId": "eef2d713-b21b-4070-afa5-d68950fb798b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "--------------------------------- START OF EPOCH [ 1 ] >>> LR =  0.0005 ---------------------------------\n",
            "\n",
            "\u001b[32m* PROGRESS IN EPOCH 1 :   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}