{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizjwh/groupIAI_5take2/blob/main/4%20New%20Model/Signlanguage_new_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xZDoFJBY3iG_"
      },
      "cell_type": "markdown",
      "source": [
        "# Classify American Sign Language Alphabet\n",
        "\n",
        "In this notebook we will learn how to classify american sign language alphabet using Pytorch and deep learning networks, We will use a pre-trained model from PyTorch models zoo and we will retrain the last parts of the network on our problem.\n",
        "We will use the python and GPU environment in Watson studio for faster training, Without having to go anywhere else we will be able to download, explore, built, and train our model.\n"
      ]
    },
    {
      "metadata": {
        "id": "vuA0_EIL3iHA"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/IBM/ASL-Pytorch/master/images/asl.jpg\" alt=\"asl\" width=\"80%%\" height=\"80%\"/>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OO97JA-g3iHB"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use PyTorch to build and train a deep learning model to classify images to 29 classes (26 ASL alphabet, space, Del, and nothing) which can be used later to help Deaf peope communicate with other and maybe with computers as well. We will use a pre-trained mobile network and we will define our classifier and connect it to network, then train this classifier along with some of the last blocks of the network on our dataset.\n",
        "\n",
        "In this notebook we will:\n",
        "\n",
        "1- Obtain dataset from Kaggle.  \n",
        "2- explore data and define transformers to preprocess images before training.  \n",
        "3- define our classifier to have an output layer of 29 outputs.  \n",
        "4- train the last blocks of the network along with the classifier we defined.  \n",
        "5- test the model we trained.  \n",
        "\n",
        "This notebook uses python 3.6 + GPU environment which allow us to do the whole process and train complex model in the same place which is a notebook in Watson studio.\n",
        "Learn more about available environments <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/gpu-environments.html\"> Watson Studio environments.</a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xSSXufw03iHB"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning Goals\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "\n",
        "* Download and prepare data in Watson studio.  \n",
        "* Download and retrain pre-trained PyTorch models on your problem.  \n",
        "* Train and test your model."
      ]
    },
    {
      "metadata": {
        "id": "O4zCl3363iHB"
      },
      "cell_type": "markdown",
      "source": [
        "# Contents  \n",
        "1- [Setting up the environment and fetching the dataset](#1.-Setting-Up-The-Environment).  <br>\n",
        "2- [Explore dataset folders and show random sample](#2.-Explore-Data).  <br>\n",
        "3- [preprocess images by transforms and load them to dataloaders](#3.-Process-and-load-images).  <br>\n",
        "4- [Choose a pre-trained model and retrain](#4.-Customize-our-network).  <br>\n",
        "5- [Try the model before any training](#5.-Try-the-model-before-any-training).   <br>\n",
        "6- [Configure training parameters and start the training](#6.-Configure-training-parameters-and-start-the-training).  <br>\n",
        "7- [Test your trained model on random image](#7.-Test-The-Model-After-Training).    <br>\n",
        "8- [Summary and Resources](#8.-Summary-and-Resources).  <br>"
      ]
    },
    {
      "metadata": {
        "id": "SulqXKgB3iHC"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up The Environment And Fetching Data\n",
        "\n",
        "## 1.1. Set up the notebook   \n",
        "#### if you are already in the notebook with GPU environment please ignore this part\n",
        "\n",
        "We will use Watson studio to run this notebook, Watson studio is available through IBM cloud or IBM Cloud Paks For Data.\n",
        "\n",
        "* To start you must have IBM cloud account, Get your for free <a href=\"https://cloud.ibm.com/registration\">Here.</a>   \n",
        "* From the catalog create a <a href=\"https://cloud.ibm.com/catalog/services/watson-studio\">watson studio</a> with standard plan or more and make sure the region is dallas to enable GPU environment.  \n",
        "* create a watson studio project <a href=\"https://www.youtube.com/watch?v=-CUi8GezG1I\">see tutorial here.</a>  \n",
        "* create a new notebook from watson studio with GPU support.    \n",
        "    * the notebook should have a GPU support <a href=\"https://www.youtube.com/watch?v=RNIWtpnNBoo\">click here to learn how to create a notebook with GPU support.</a>\n",
        "    * Use the From URL option and use this URL: https://github.com/IBM/ASL-Pytorch/blob/master/ASL-GPU.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "0bZIdA6M3iHC"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Fetching data\n",
        "\n",
        "In this section, we will get the required missing packages, download, and unzip the dataset.\n",
        "\n",
        "### Get Kaggle CLI and torch vision package\n",
        "\n",
        "First, we will need to install Kaggle CLI and torch vision package, For that, we use the exclamation mark to run command in the kernel."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleat folders if we want a refresh\n",
        "'''\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('Full Mobility', ignore_errors=True)\n",
        "'''"
      ],
      "metadata": {
        "id": "XPlzQ6PamMTP",
        "outputId": "dc4add7b-fcf9-4248-dfc9-31fb92a86c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport shutil\\n\\nshutil.rmtree('Full Mobility', ignore_errors=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_MmY7_p3iHC",
        "outputId": "fb703a39-5bbe-4c5d-ccf0-5892e856bbb4"
      },
      "cell_type": "code",
      "source": [
        "#install torchvision and kaggle\n",
        "!pip install torchvision\n",
        "!pip install kaggle\n",
        "!pip install tqdm\n",
        "!pip install colorama\n",
        "!pip install split-folders[full]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "07MV-lTs3iHD"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and unzip the dataset\n",
        "\n",
        "We will use a dataset of 87.0k ASL Alphabet images (JPG) and 29 categories (26 ASL alphabet, space, Del, and nothing) offered by amarinderplasma on Kaggle. For more about the dataset <a href=\"https://www.kaggle.com/amarinderplasma/alphabets-sign-language\"> ASL Images.</a>\n",
        "\n",
        "* In this cell you should import your Kaggle account credentials to be able to download the data through Kaggle CLI. To get your credentials <a href=\"https://www.kaggle.com/docs/api\">Kaggle API key.</a>\n",
        "* Please note the line should be run as it is as one line to make sure all commands are started in the same kernel."
      ]
    },
    {
      "metadata": {
        "id": "E9gYOE073iHD"
      },
      "cell_type": "code",
      "source": [
        "# download dataset and extract it\n",
        "! export KAGGLE_USERNAME=\"emmet454\" && export KAGGLE_KEY=\"ee00fbc0728a71f5c5f712029e3ef004\" && kaggle datasets download --force --unzip emilyburt/intro-to-ai\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Following stuff might not be relivent"
      ],
      "metadata": {
        "id": "aG7X8YUzvi6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders"
      ],
      "metadata": {
        "id": "Bg99dJXdm8MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = 'Dataset/Full Mobility/Left hand'\n",
        "\n",
        "#split with a ratio (train, val, test)\n",
        "splitfolders.ratio(input_folder, output='Full Mobility/Left hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Full Mobility/Right hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Full Mobility/Right hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Restricted mobility/Left hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Restricted Mobility/Left hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n",
        "\n",
        "input_folder = 'Dataset/Restricted mobility/Right hand'\n",
        "\n",
        "#split with a ratio\n",
        "splitfolders.ratio(input_folder, output='Restricted Mobility/Right hand/',\n",
        "                  seed=42, ratio=(.8, .15, .05),\n",
        "                  group_prefix=None)\n"
      ],
      "metadata": {
        "id": "YshIuTosjbaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FbRzWQWr3iHE"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Process and load images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "jSjPhlO9P2YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, models ,datasets\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "def create_dataset(root_dir):\n",
        "    \"\"\"\n",
        "    Creates an ImageFolder dataset from the specified root directory.\n",
        "    \"\"\"\n",
        "    data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.RandomRotation(30),\n",
        "                                          transforms.RandomHorizontalFlip(p=0.3),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "    dataset = datasets.ImageFolder(root_dir, transform=data_transforms)\n",
        "    return dataset\n",
        "\n",
        "def combine_datasets(root_folders):\n",
        "    \"\"\"\n",
        "    Combines datasets from multiple root folders into one dataset.\n",
        "    \"\"\"\n",
        "    datasets = [create_dataset(folder) for folder in root_folders]\n",
        "    combined_dataset = ConcatDataset(datasets)\n",
        "\n",
        "    # Extract class-to-index mappings from individual datasets and merge them\n",
        "    class_to_idx = {}\n",
        "    for dataset in datasets:\n",
        "        class_to_idx.update(dataset.class_to_idx)\n",
        "\n",
        "    return combined_dataset, class_to_idx\n",
        "\n",
        "\n",
        "def main():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/train',\n",
        "                    '/content/Restricted Mobility/Right hand/train',\n",
        "                    '/content/Full Mobility/Left hand/train',\n",
        "                    '/content/Full Mobility/Right hand/train']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "def main_val():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/val',\n",
        "                    '/content/Restricted Mobility/Right hand/val',\n",
        "                    '/content/Full Mobility/Left hand/val',\n",
        "                    '/content/Full Mobility/Right hand/val']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "def main_test():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Restricted Mobility/Left hand/test',\n",
        "                    '/content/Restricted Mobility/Right hand/test',\n",
        "                    '/content/Full Mobility/Left hand/test',\n",
        "                    '/content/Full Mobility/Right hand/test']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "combined_train_dataset = main()\n",
        "combined_val_dataset = main_val()\n",
        "combined_test_dataset = main_test()"
      ],
      "metadata": {
        "id": "UztNECipL02c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uMpatdjX3iHF"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Customize our network\n",
        "\n",
        "## 4.1. Choose a Pretrained model\n",
        "choose a pre-trained model you think would have good accuracy for the problem and freeze all layers to make sure not all the network is trained during the training process.   \n",
        "A list of Pytorch model zoo can be found <a href=\"https://pytorch.org/docs/stable/torchvision/models.html\">here.</a>  \n",
        "We will also print the model classifier to check the number of input neurons to consider when creating our classifier."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "o-SKchRd3iHF"
      },
      "cell_type": "code",
      "source": [
        "# choose a pretrained model to start with check options here: https://pytorch.org/vision/stable/models.html\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Freeze parameters of the tarined network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#print the model to check the classifer and change it\n",
        "print (model.classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESWNtEZn3iHF"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2. Choose which layers to train\n",
        "In the next cell, we will unfreeze some of the last blocks of our network and we will define our custom classifier to have 29 outputs then we will attach it to the model.  \n",
        "We will also choose the optimizer we want and define the learning rate for it."
      ]
    },
    {
      "metadata": {
        "id": "drBHDLF23iHF"
      },
      "cell_type": "code",
      "source": [
        "# define new classifier and append it to network but remember to have a 29-neuron output layer for our two classes.\n",
        "model.classifier= nn.Sequential(nn.Dropout(p=0.6, inplace=False),\n",
        "                                nn.Linear(in_features=1280, out_features=29, bias=True),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "\n",
        "# unlock last three blocks before the classifier(last layer).\n",
        "for p in model.features[-3:].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "# choose your loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# define optimizer to train only the classifier and the previous three block.\n",
        "optimizer = optim.Adam([{'params':model.features[-1].parameters()},\n",
        "                        {'params':model.features[-2].parameters()},\n",
        "                        {'params':model.features[-3].parameters()},\n",
        "                        {'params':model.classifier.parameters()}], lr=0.0005)\n",
        "\n",
        "# define Learning Rate scheduler to decrease the learning rate by multiplying it by 0.1 after each epoch on the data.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "#print the classifier now\n",
        "print(model.classifier)\n",
        "\n",
        "#print the whole model\n",
        "# print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1_c1HgY3iHG"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Configure training parameters and start the training  \n",
        "## 6.1. choose the number of epochs and printing intervals and move the model to the GPU."
      ]
    },
    {
      "metadata": {
        "id": "cpx2g0br3iHG"
      },
      "cell_type": "code",
      "source": [
        "#Define number of epochs through data and run the training loop\n",
        "import math\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(combined_train_dataset, batch_size=512, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(combined_test_dataset, batch_size=512)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "epochs = 5\n",
        "step = 0\n",
        "running_loss = 0\n",
        "print_every = 20\n",
        "trainlossarr=[]\n",
        "testlossarr=[]\n",
        "oldacc=0\n",
        "\n",
        "steps=math.ceil(len(combined_train_dataset)/(trainloader.batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CpwJeRo23iHG"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2. Start the training loop\n",
        "Set back and watch your model learn."
      ]
    },
    {
      "metadata": {
        "id": "mvLbQMjm3iHG"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "from colorama import Fore,Style\n",
        "import sys\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(Style.RESET_ALL)\n",
        "    print(f\"--------------------------------- START OF EPOCH [ {epoch+1} ] >>> LR =  {optimizer.param_groups[-1]['lr']} ---------------------------------\\n\")\n",
        "    for inputs, labels in tqdm(trainloader,desc=Fore.GREEN +f\"* PROGRESS IN EPOCH {epoch+1} \",file=sys.stdout):\n",
        "        model.train()\n",
        "        step += 1\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        props = model.forward(inputs)\n",
        "        loss = criterion(props, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (step % print_every == 0) or (step==steps):\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in testloader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    props = model.forward(inputs)\n",
        "                    batch_loss = criterion(props, labels)\n",
        "\n",
        "                    test_loss += batch_loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    ps = torch.exp(props)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            tqdm.write(f\"Epoch ({epoch+1} of {epochs}) ... \"\n",
        "                  f\"Step  ({step:3d} of {steps}) ... \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f} ... \"\n",
        "                  f\"Test loss: {test_loss/len(testloader):.3f} ... \"\n",
        "                  f\"Test accuracy: {accuracy/len(testloader):.3f} \")\n",
        "            trainlossarr.append(running_loss/print_every)\n",
        "            testlossarr.append(test_loss/len(testloader))\n",
        "            running_loss = 0\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "    step=0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'new_model.pth')"
      ],
      "metadata": {
        "id": "W6wQLhrC0JKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiQCz8Ax3iHH"
      },
      "cell_type": "markdown",
      "source": [
        "# 7. Test The Model After Training"
      ]
    },
    {
      "metadata": {
        "id": "GD69QwSP3iHH"
      },
      "cell_type": "code",
      "source": [
        "### try your model on some images\n",
        "%matplotlib inline\n",
        "\n",
        "#turn model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "#load some of the test data\n",
        "#test_data = datasets.ImageFolder(valid_path,transforms.Compose([transforms.ToTensor()]))\n",
        "testloader = torch.utils.data.DataLoader(combined_val_dataset, batch_size=200,shuffle=True)\n",
        "images , labels=next( iter(testloader) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NOV1uA9RuDTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egNmMp1h3iHH"
      },
      "cell_type": "code",
      "source": [
        "#Choose arandom image from 0 to 199\n",
        "index = np.random.randint(0, 199)\n",
        "test_img=images[index]\n",
        "\n",
        "#show choosed image\n",
        "t=transforms.ToPILImage()\n",
        "plt.imshow(t(test_img))\n",
        "\n",
        "#normalize image as in the training data\n",
        "t_n=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "test_img=t_n(test_img).unsqueeze(0)\n",
        "\n",
        "#classify image using our model\n",
        "res = torch.exp(model(test_img))\n",
        "\n",
        "#invert class_to_idx keys to values and viceversa.\n",
        "classes=combined_train_dataset.class_to_idx\n",
        "classes = {value:key for key, value in classes.items()}\n",
        "\n",
        "print(f\"image number {index}\")\n",
        "print(\"---------------------\")\n",
        "\n",
        "#print real class\n",
        "print(\"label:\",classes[labels[index].item()])\n",
        "\n",
        "#print predicted class\n",
        "print(\"prediction:\", classes[res.argmax().item()])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether we have a GPU.  Use it if we do.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=combined_dataset,\n",
        "                                          batch_size=10,\n",
        "                                          shuffle=True)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    # Do one pass over the test data.\n",
        "    # In the test phase, don't need to compute gradients (for memory efficiency)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        accuracies = []\n",
        "        losses = []\n",
        "        for images, labels in test_loader:\n",
        "            #Convert image pixels to vector\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(images).squeeze((-1, -2))\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, labels)\n",
        "            losses.append(loss.item())  # Append the loss to the list\n",
        "\n",
        "            # Compute total correct so far\n",
        "            predicted = torch.argmax(logits, -1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Calculate accuracy for the current batch\n",
        "            batch_accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
        "            accuracies.append(batch_accuracy)\n",
        "\n",
        "        print(f'Test accuracy : {100 * correct / total} %')\n",
        "\n",
        "        # Plot accuracy graph\n",
        "        plt.plot(accuracies)\n",
        "        plt.title('Accuracy Over Test Set Batches')\n",
        "        plt.xlabel('Batch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot loss graph\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('Batch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss Over Test Set Batches')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return total\n",
        "\n",
        "# Run training\n",
        "total = test(1)\n",
        "print(total)"
      ],
      "metadata": {
        "id": "B_BnRyNEfbZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLAocIdp3iHH"
      },
      "cell_type": "markdown",
      "source": [
        "# 8. Summary and Resources\n",
        "\n",
        "You made it, Thanks for completing the notebook.  \n",
        "* In this notebook we learned how to do the whole process of model building and training on notebook leveraging the power GPU environment to train deep learning models.\n",
        "* Don't forget to get back and try different parameters or different models.\n",
        "\n",
        "## Resources\n",
        "* <a href=\"https://dataplatform.cloud.ibm.com/gallery\">Other Great notebooks.</a>  \n",
        "* <a href=\"https://www.python.org/\">Official Python website.</a>  \n",
        "* <a href=\"https://pytorch.org/\">Official PyTorch website</a>  \n",
        "* <a href=\"https://cloud.ibm.com/registration\">Get started today on IBM Cloud for free!</a>   \n",
        "* <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/gpu-environments.html\">Learn more about GPU environments.</a>   "
      ]
    },
    {
      "metadata": {
        "id": "NyZVBzML3iHH"
      },
      "cell_type": "markdown",
      "source": [
        "## Author\n",
        "\n",
        "Mostafa Abdelaleem is a Developer Advocate at IBM who contributes to AI and data science community in order to democratize them.\n",
        "\n",
        "Copyright Â© 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}