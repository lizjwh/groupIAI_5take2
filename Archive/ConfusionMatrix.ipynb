{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizjwh/groupIAI_5take2/blob/main/3%20Testing%20on%20Original%20Model/ConfusionMatrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test script\n",
        "This notebook will test the OGmodel on the full and restricted mobility datasets.\n"
      ],
      "metadata": {
        "id": "r4UelXQ2AGzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install torchvision and kaggle\n",
        "!pip install torchvision\n",
        "!pip install kaggle\n",
        "!pip install tqdm\n",
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziWvctBTBkn8",
        "outputId": "3e92c201-2a95-4029-fa85-595563541613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import  needed libraries and check the used gpu\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torchvision import transforms, models ,datasets\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc"
      ],
      "metadata": {
        "id": "_tCw20tAIyhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on our restricted mobility dataset\n",
        "First, import dataset from Emily's kaggle"
      ],
      "metadata": {
        "id": "osbyhlPfAd5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E94VXYBt_8n_"
      },
      "outputs": [],
      "source": [
        "! export KAGGLE_USERNAME=\"emmet454\" && export KAGGLE_KEY=\"ee00fbc0728a71f5c5f712029e3ef004\" && kaggle datasets download --force --unzip emilyburt/intro-to-ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"File path and subfolders\")\n",
        "! pwd && ls\n",
        "print(\"\\nFolders and files in the Dataset directory:\")\n",
        "! cd Dataset && ls"
      ],
      "metadata": {
        "id": "hxV1ijTkAo1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the model with 29 classes (this block is copied from tutorial)"
      ],
      "metadata": {
        "id": "LV-wzQRNPWwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "# Freeze parameters of the tarined network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# define new classifier and append it to network but remember to have a 29-neuron output layer for our two classes.\n",
        "model.classifier= nn.Sequential(nn.Dropout(p=0.6, inplace=False),\n",
        "                                nn.Linear(in_features=1280, out_features=29, bias=True),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "\n",
        "# unlock last three blocks before the classifier(last layer).\n",
        "for p in model.features[-3:].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "# choose your loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# define optimizer to train only the classifier and the previous three block.\n",
        "optimizer = optim.Adam([{'params':model.features[-1].parameters()},\n",
        "                        {'params':model.features[-2].parameters()},\n",
        "                        {'params':model.features[-3].parameters()},\n",
        "                        {'params':model.classifier.parameters()}], lr=0.0005)\n",
        "\n",
        "# define Learning Rate scheduler to decrease the learning rate by multiplying it by 0.1 after each epoch on the data.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "metadata": {
        "id": "5URjJBnCBOt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the OG model - you must make sure the OGModelWeights file is uploaded to the current Colab session\n"
      ],
      "metadata": {
        "id": "ypv-hMI-PT3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lizjwh/groupIAI_5take2\n",
        "device = torch.device(\"cuda\")\n",
        "model.load_state_dict(torch.load('groupIAI_5take2/OGModelWeights.pth'))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ZAekb3V7PEZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create array of all images from restricted mobility and display the first image.\n",
        "\n"
      ],
      "metadata": {
        "id": "rZcbgXO8VMUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Restricted_ASL=np.array(glob.glob('/content/Dataset/Restricted mobility/*/*/*'))\n",
        "#print(Restricted_ASL)\n",
        "im = plt.imread(Restricted_ASL[0]) #they seem to be rotated\n",
        "plt.imshow(im,interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AchB9LOSVOpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a dataloader"
      ],
      "metadata": {
        "id": "ft2-HQMSdZtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#These functions allow each folder to be combined\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "def create_dataset(root_dir):\n",
        "    \"\"\"\n",
        "    Creates an ImageFolder dataset from the specified root directory.\n",
        "    \"\"\"\n",
        "    data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "    dataset = datasets.ImageFolder(root_dir, transform=data_transforms)\n",
        "    return dataset\n",
        "\n",
        "def combine_datasets(root_folders):\n",
        "    \"\"\"\n",
        "    Combines datasets from multiple root folders into one dataset.\n",
        "    \"\"\"\n",
        "    datasets = [create_dataset(folder) for folder in root_folders]\n",
        "    combined_dataset = ConcatDataset(datasets)\n",
        "\n",
        "    # Extract class-to-index mappings from individual datasets and merge them\n",
        "    class_to_idx = {}\n",
        "    for dataset in datasets:\n",
        "        class_to_idx.update(dataset.class_to_idx)\n",
        "\n",
        "    return combined_dataset, class_to_idx\n",
        "\n",
        "def main():\n",
        "  #choose which folders to include in test here\n",
        "    root_folders = ['/content/Dataset/Restricted mobility/Left hand',\n",
        "                    '/content/Dataset/Restricted mobility/Right hand',\n",
        "                    '/content/Dataset/Full Mobility/Left hand',\n",
        "                    '/content/Dataset/Full Mobility/Right hand']\n",
        "\n",
        "    combined_dataset, combined_dataset.class_to_idx = combine_datasets(root_folders)\n",
        "\n",
        "    print(f\"Class to index mapping: {combined_dataset.class_to_idx}\")\n",
        "    return combined_dataset\n",
        "\n",
        "combined_dataset = main()\n",
        "\n",
        "\n",
        "#define the transform for the dataset\n",
        "#test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "#                                      transforms.ToTensor(),\n",
        "#                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
        " #                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "#transform the data\n",
        "#test_data = datasets.ImageFolder(path,test_transforms)\n",
        "#path = '/content/Dataset/Restricted mobility/Right hand'\n",
        "#test_data.append(datasets.ImageFolder(path,test_transforms))\n",
        "#path = '/content/Dataset/Full Mobility/Left hand'\n",
        "#test_data.append(datasets.ImageFolder(path,test_transforms))\n",
        "#path = '/content/Dataset/Full Mobility/Right hand'\n",
        "#test_data.append(datasets.ImageFolder(path,test_transforms))\n",
        "#print(f\"class to index mapping: {test_data.class_to_idx}\")\n",
        "\n",
        "#load some of the test data\n",
        "testloader = torch.utils.data.DataLoader(combined_dataset, batch_size=50, shuffle=True)\n",
        "images, labels = next(iter(testloader))\n",
        "print(labels)\n",
        "#print(images.size())"
      ],
      "metadata": {
        "id": "MzvhpS24daE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use model to make prediction on image"
      ],
      "metadata": {
        "id": "cyErzTgCk_MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#select random image from 0-(batch_size)\n",
        "index = np.random.randint(0,50)\n",
        "test_img = images[index]\n",
        "t = transforms.ToPILImage()\n",
        "plt.imshow(t(test_img))\n",
        "\n",
        "#normalize image as in the training data\n",
        "t_n=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "test_img=t_n(test_img).unsqueeze(0).cuda()\n",
        "\n",
        "#classify image using our model\n",
        "#print(test_img.size())\n",
        "res = torch.exp(model(test_img))\n",
        "#print(\"res:\",res)\n",
        "#invert class_to_idx keys to values and viceversa.\n",
        "classes = combined_dataset.class_to_idx\n",
        "classes = {value:key for key, value in classes.items()}\n",
        "\n",
        "print(f\"image number {index}\")\n",
        "print(\"---------------------\")\n",
        "\n",
        "#print real class\n",
        "print(\"label:\",classes[labels[index].item()])\n",
        "\n",
        "#print predicted class\n",
        "#print(\"prediction index:\",res.argmax().item())\n",
        "print(\"prediction letter:\", classes[res.argmax().item()])\n"
      ],
      "metadata": {
        "id": "henfC5t_lDTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix\n",
        "https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
      ],
      "metadata": {
        "id": "1h4QV1fMB7x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "# Check whether we have a GPU.  Use it if we do.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batchsize = 10\n",
        "test_loader = torch.utils.data.DataLoader(dataset=combined_dataset,\n",
        "                                          batch_size=batchsize,\n",
        "                                          shuffle=True)\n",
        "def test(classes):\n",
        "      pred = []\n",
        "      true = []\n",
        "\n",
        "      images, labels = next(iter(testloader))\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      for index in range(0, batchsize):\n",
        "          test_img = images[index]\n",
        "\n",
        "          t = transforms.ToPILImage()\n",
        "          #normalize image as in the training data\n",
        "          t_n=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "          test_img=t_n(test_img).unsqueeze(0).cuda()\n",
        "\n",
        "          #classify image using our model\n",
        "          res = torch.exp(model(test_img))\n",
        "          #print(\"res:\",res)\n",
        "\n",
        "          #Save prediction\n",
        "          print(\"prediction:\", classes[res.argmax().item()])\n",
        "          pred.extend(classes[res.argmax().item()])\n",
        "\n",
        "          #save truth\n",
        "          print(\"label:\",classes[labels[index].item()])\n",
        "          true.extend(classes[labels[index].item()])\n",
        "\n",
        "      return pred, true\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plotConfusionMatrix(pred, true, classes):\n",
        "    cf_matrix = confusion_matrix(true,pred)\n",
        "    print(cf_matrix)\n",
        "\n",
        "    classes = list(classes.values())\n",
        "    print(classes)\n",
        "\n",
        "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:,None], index=[i for i in classes], columns=[i for i in classes])\n",
        "    plt.figure(figsize=(12,7))\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "\n",
        "#invert class_to_idx keys to values and viceversa.\n",
        "classes = combined_dataset.class_to_idx\n",
        "classes = {value:key for key, value in classes.items()}\n",
        "\n",
        "pred, true = test(classes)\n",
        "print(pred, true)\n",
        "plotConfusionMatrix(pred, true, classes)\n"
      ],
      "metadata": {
        "id": "6qpAVI4GQ7ky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}